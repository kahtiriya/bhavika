{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahtiriya/bhavika/blob/main/frauddetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9-Fre4FMda"
      },
      "source": [
        "# **Group ID: 27**\n",
        "1.   Student 1: **Bhavika Kathiriya** and **u2555134**\n",
        "2.   Student 2: **Nirav Saileshbhai Umaretiya** and **u2613177**\n",
        "3.   Student 3: **Anufi Prasad** and **u2564175**\n",
        "\n",
        "---\n",
        "#####Module leader: **Dr Amin Karami** & **Nadeem Qazi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdMZR-9QTwG3"
      },
      "source": [
        "\n",
        "# **Initiate and Configure Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0p88Xtw_3-",
        "outputId": "37c43318-47f7-4b60-a9d0-453d63664976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=42c9011e9ebe660dd38cb85766847c9cca26bfe020403eed3103b7ede85dc252\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boxKbVxtfi4A"
      },
      "outputs": [],
      "source": [
        "# Initiate SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('BankApp').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2CZVl6TOQX"
      },
      "source": [
        "\n",
        "# **Task 1: Data Loading and Preprocessing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Y5bzK9mvo3"
      },
      "source": [
        "### Data Explanation:\n",
        "\n",
        "Here, we choose **Bank Account Fraud** dataset.\n",
        "\n",
        "It comprises a total of 6 different synthetic bank account fraud tabular datasets and datasets published at **NeurIPS 2022**.\n",
        "\n",
        "BAF is a realistic, complete, and robust test bed to evaluate novel and existing methods in ML and fair ML, and the first of its kind!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc5GMMvdh5jD",
        "outputId": "210eff1d-be18-4f5d-aa0f-3fd0c8523483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google drive mount here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNLHTIcyY9Hp"
      },
      "outputs": [],
      "source": [
        "# Bank Data is load from Drive\n",
        "\n",
        "df = spark.read.load(\"/content/drive/MyDrive/combined.csv\", format=\"csv\", header= True, inferschema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKaoC9Z-gfIf",
        "outputId": "5307add4-b53c-4baa-f912-2bc9e6e820a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------+--------------+----------+-------------------+---------------+------------------+\n",
            "|      date|txVolume(USD)|txCount|marketcap(USD)|price(USD)|exchangeVolume(USD)|activeAddresses|medianTxValue(USD)|\n",
            "+----------+-------------+-------+--------------+----------+-------------------+---------------+------------------+\n",
            "|2018-08-12|5324506.83294|    284|    1116316004|     11.69|           23290300|            212|     283.703661356|\n",
            "|2018-08-11|8842440.96721|    480|    1156716831|     12.11|           29253100|            321|     349.972279784|\n",
            "|2018-08-10| 5336434.1335|    394|    1219512788|     12.77|           37026400|            294|     240.996396601|\n",
            "|2018-08-09|11809334.8381|    365|    1161589120|     12.16|           41389000|            251|     269.626930125|\n",
            "|2018-08-08|8283783.58688|    881|    1253974051|     13.13|           44898500|            262|         6562.6366|\n",
            "|2018-08-07| 7026460.3368|    780|    1304555493|     13.66|           45119500|            281|      4453.8845264|\n",
            "|2018-08-06|5523862.61387|    755|    1307338226|     13.69|           46799600|            265|        5066.36782|\n",
            "|2018-08-05| 678949.33254|    302|    1323679385|     13.86|           43000300|            218|     380.290137381|\n",
            "|2018-08-04|2195490.58211|    372|    1347937352|     14.11|           60865200|            262|     345.191898261|\n",
            "|2018-08-03|7283737.75081|   1037|    1345579816|     14.09|           70449300|            481|     940.496639146|\n",
            "|2018-08-02|11934972.0463|   8958|    1301912488|     13.63|           50481200|           8292|       247.2256324|\n",
            "|2018-08-01|6311774.97394|    842|    1315172782|     13.77|           53833700|            313|     4070.04483439|\n",
            "|2018-07-31|10026966.5673|    797|    1261822999|     13.21|           66537000|            381|         6602.2259|\n",
            "|2018-07-30|6904769.31097|    919|    1332179975|     13.95|           35991600|            758|          6976.395|\n",
            "|2018-07-29|32041298.3138|   2509|    1394301312|      14.6|           56547100|           1660|           7301.46|\n",
            "|2018-07-28|26575271.4153|   2873|    1339620480|     14.03|           75159904|           1772|       6296.631731|\n",
            "|2018-07-27|13553918.5268|   1550|    1279151488|     13.39|           66040700|           1107|       2419.256996|\n",
            "|2018-07-26|12549366.5857|   1607|    1234165120|     12.92|           61881400|            827|         5229.7576|\n",
            "|2018-07-25|4224614.26536|    590|    1154459904|     12.09|           72083200|            410|         496.18569|\n",
            "|2018-07-24|2221910.26641|    412|    1148270592|     12.02|           46762700|            369|     290.831042284|\n",
            "+----------+-------------+-------+--------------+----------+-------------------+---------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Here we Print Bank Datasets\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "5WzQ3RLtCucA",
        "outputId": "bd063833-1843-4908-defb-30f06264e2e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `farud_bool` cannot be resolved. Did you mean one of the following? [`CUST_ID`, `BALANCE`, `PAYMENTS`, `TENURE`, `PURCHASES`].;\n'Aggregate ['farud_bool], ['farud_bool, count(1) AS count#184L]\n+- Relation [CUST_ID#17,BALANCE#18,BALANCE_FREQUENCY#19,PURCHASES#20,ONEOFF_PURCHASES#21,INSTALLMENTS_PURCHASES#22,CASH_ADVANCE#23,PURCHASES_FREQUENCY#24,ONEOFF_PURCHASES_FREQUENCY#25,PURCHASES_INSTALLMENTS_FREQUENCY#26,CASH_ADVANCE_FREQUENCY#27,CASH_ADVANCE_TRX#28,PURCHASES_TRX#29,CREDIT_LIMIT#30,PAYMENTS#31,MINIMUM_PAYMENTS#32,PRC_FULL_PAYMENT#33,TENURE#34] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-16ae25786819>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"farud_bool\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GroupedData\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `farud_bool` cannot be resolved. Did you mean one of the following? [`CUST_ID`, `BALANCE`, `PAYMENTS`, `TENURE`, `PURCHASES`].;\n'Aggregate ['farud_bool], ['farud_bool, count(1) AS count#184L]\n+- Relation [CUST_ID#17,BALANCE#18,BALANCE_FREQUENCY#19,PURCHASES#20,ONEOFF_PURCHASES#21,INSTALLMENTS_PURCHASES#22,CASH_ADVANCE#23,PURCHASES_FREQUENCY#24,ONEOFF_PURCHASES_FREQUENCY#25,PURCHASES_INSTALLMENTS_FREQUENCY#26,CASH_ADVANCE_FREQUENCY#27,CASH_ADVANCE_TRX#28,PURCHASES_TRX#29,CREDIT_LIMIT#30,PAYMENTS#31,MINIMUM_PAYMENTS#32,PRC_FULL_PAYMENT#33,TENURE#34] csv\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"farud_bool\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Avtu6ZGBZGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ed08ba96-bcc5-44ed-a3aa-202ec602435a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "fraud_bool does not exist. Available: CUST_ID, BALANCE, BALANCE_FREQUENCY, PURCHASES, ONEOFF_PURCHASES, INSTALLMENTS_PURCHASES, CASH_ADVANCE, PURCHASES_FREQUENCY, ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, CASH_ADVANCE_FREQUENCY, CASH_ADVANCE_TRX, PURCHASES_TRX, CREDIT_LIMIT, PAYMENTS, MINIMUM_PAYMENTS, PRC_FULL_PAYMENT, TENURE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d672e6c9256e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Handling Missing Values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fraud_bool\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fraud_bool\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: fraud_bool does not exist. Available: CUST_ID, BALANCE, BALANCE_FREQUENCY, PURCHASES, ONEOFF_PURCHASES, INSTALLMENTS_PURCHASES, CASH_ADVANCE, PURCHASES_FREQUENCY, ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, CASH_ADVANCE_FREQUENCY, CASH_ADVANCE_TRX, PURCHASES_TRX, CREDIT_LIMIT, PAYMENTS, MINIMUM_PAYMENTS, PRC_FULL_PAYMENT, TENURE"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "# Handling Missing Values\n",
        "imputer = Imputer(inputCols=[\"fraud_bool\"], outputCols=[\"fraud_bool\"])\n",
        "data = imputer.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7XiFwTsByGM"
      },
      "outputs": [],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcZfiIcq1Qxn"
      },
      "outputs": [],
      "source": [
        "# Import VectorAssembler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Here, we select some column as input column and transform featues column into single vector\n",
        "inputCols= [\"income\",\"name_email_similarity\",\"days_since_request\",\"intended_balcon_amount\",\"velocity_6h\",\"session_length_in_minutes\"]\n",
        "vecAss = VectorAssembler(inputCols = inputCols, outputCol = \"features\")\n",
        "\n",
        "# Display all column in datasets and vector assember features column.\n",
        "finalData = vecAss.transform(df).select('fraud_bool',\"income\",\"name_email_similarity\",\"days_since_request\",\"intended_balcon_amount\",\"velocity_6h\",\"session_length_in_minutes\",'payment_type','features')\n",
        "finalData.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLxWUqKxj1hQ"
      },
      "outputs": [],
      "source": [
        "# Here, We are import library for RandomForestClassifier,MulticlassClassificationEvaluator and StandardScaler.\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "# use of the VectorAssembler() function to transform our data into vector of features column.\n",
        "rfFinalData = vecAss.transform(df)\n",
        "\n",
        "# use of StandardScaler() function to scale our data.\n",
        "stanardscalar=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
        "data=stanardscalar.fit(rfFinalData).transform(rfFinalData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAmp0OXlkRnQ"
      },
      "outputs": [],
      "source": [
        "# Display Scaled_features and fraud_bool (dependent variable).\n",
        "assembledData = data.select(\"Scaled_features\",\"fraud_bool\")\n",
        "assembledData.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqWzxRdMkVX5"
      },
      "outputs": [],
      "source": [
        "# use of randomSplit() function to data split into train data and test data.\n",
        "rfTrain, rfTest = assembledData.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzEjHVm3kYSK"
      },
      "outputs": [],
      "source": [
        "# Initialize RandomForestClassifier() function with parameter fraud_bool and Scaled_features.\n",
        "randomForestClassifier = RandomForestClassifier(labelCol=\"fraud_bool\",featuresCol=\"Scaled_features\",numTrees=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_wK7-v6ka9H"
      },
      "outputs": [],
      "source": [
        "# Generate model using train data.\n",
        "model = randomForestClassifier.fit(rfTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qi_vwkLuTEq"
      },
      "outputs": [],
      "source": [
        "# Here prediction using rf_test data.\n",
        "predictionTest = model.transform(rfTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgGKUAFx4dxt"
      },
      "outputs": [],
      "source": [
        "# Select (prediction, true label) and compute test error.\n",
        "evaluator = MulticlassClassificationEvaluator( labelCol=\"fraud_bool\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avagK8R2Jxka"
      },
      "source": [
        "# **Task 3 - Model Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifm8Du7C5Nbq"
      },
      "outputs": [],
      "source": [
        "# Import library of CrossValidator and ParamGridBuilder.\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Define parameter grid.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(randomForestClassifier.numTrees, [10, 20, 30]) \\\n",
        "    .addGrid(randomForestClassifier.maxDepth, [5, 10, 15]) \\\n",
        "    .build()\n",
        "\n",
        "# Define CrossValidator with Random Forest, parameter grid, and evaluator.\n",
        "crossval = CrossValidator(estimator=randomForestClassifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRzzg-XftSvF"
      },
      "outputs": [],
      "source": [
        "# Run cross-validation, and choose the best set of parameters\n",
        "cvModel = crossval.fit(data.limit(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmHx1z6RqQ9I"
      },
      "outputs": [],
      "source": [
        "# Here prediction for best results.\n",
        "prediction = cvModel.transform(data.limit(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfjchd6yqoYq"
      },
      "outputs": [],
      "source": [
        "selected = prediction.select(\"*\").limit(10)\n",
        "for row in selected.collect():\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L47fyG8kE6x"
      },
      "outputs": [],
      "source": [
        "# find the best parameters and Model\n",
        "bestModel = cvModel.bestModel\n",
        "evaluator = cvModel.getEvaluator()\n",
        "bestScore = max(cvModel.avgMetrics)\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\")\n",
        "print(\"numTrees:\", bestModel.getNumTrees)\n",
        "print(\"maxDepth:\", bestModel.getMaxDepth)\n",
        "print(\"BestScore:\", bestScore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX277ub716W_"
      },
      "source": [
        "# **Task 4 - Model Evaluation and Accuracy Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9R3j0T4ugVT"
      },
      "outputs": [],
      "source": [
        "# Evaluate the accuracy.\n",
        "accuracy_RF= evaluator.evaluate(predictionTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eqAhPeKwY3L"
      },
      "outputs": [],
      "source": [
        "print (\"Accuracy:  \", round((accuracy_RF) * 100,2))\n",
        "print (\"Error: \", round((1.0 - accuracy_RF) * 100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA6pB16xLoTP"
      },
      "source": [
        "# **Task 5 - Results Visualization or Printing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W07M792G2U3S"
      },
      "outputs": [],
      "source": [
        "# Import library matplotlib.pyplot and numpy for Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdIynZiz3Ssh"
      },
      "outputs": [],
      "source": [
        "paramGrid = cvModel.getEstimatorParamMaps()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOSqNS5h3V_M"
      },
      "outputs": [],
      "source": [
        "avgMetrics = cvModel.avgMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK8Lz0yu3bPY"
      },
      "outputs": [],
      "source": [
        "# set numTrees_values and maxDepth_values of each tree using for loop.\n",
        "numTrees_values = [params[randomForestClassifier.numTrees] for params in paramGrid]\n",
        "maxDepth_values = [params[randomForestClassifier.maxDepth] for params in paramGrid]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zewD8rP73j7E"
      },
      "outputs": [],
      "source": [
        "# set parameter X,Y and Title.\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(numTrees_values, avgMetrics, marker='o', linestyle='-')\n",
        "plt.xlabel('numTrees')\n",
        "plt.ylabel('Average Metric')\n",
        "plt.title('Average Metric vs numTrees')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using different method with same dataset"
      ],
      "metadata": {
        "id": "o16uO7FfSLPU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uufqoAnuWUVF"
      },
      "outputs": [],
      "source": [
        "#Add Library for LinearRegression, RegressionEvaluator, Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lti-mBxmWCFe"
      },
      "outputs": [],
      "source": [
        "# Set Train and Test Data\n",
        "LTrainData, LTestData = assembledData.randomSplit([0.7, 0.3], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPPBEZD0WKBQ"
      },
      "outputs": [],
      "source": [
        "# Init LinearRegression\n",
        "lr = LinearRegression(labelCol=\"fraud_bool\",featuresCol=\"Scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF9lCccCWM7E"
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "pipeline = Pipeline(stages=[lr])\n",
        "LModel = pipeline.fit(LTrainData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guL3_rL2WP-5"
      },
      "outputs": [],
      "source": [
        "# Transform Test Data and get Predictions\n",
        "Lpredictions = LModel.transform(LTestData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9HXaHqngmrO"
      },
      "source": [
        "# **Task 3 - Model Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGCL2mKzg3bT"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sji1Ih_kmQF-"
      },
      "outputs": [],
      "source": [
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=param_grid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fa9WdKHmb5D"
      },
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "cv_model = crossval.fit(data.limit(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8YiZtBvu0F8"
      },
      "outputs": [],
      "source": [
        "# Best model from cross-validation\n",
        "best_model = cv_model.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTdf00jNu9mb"
      },
      "outputs": [],
      "source": [
        "# Print the best regularization parameter\n",
        "print(\"Best regularization parameter:\", best_model._java_obj.getRegParam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W3iE1Fhwwen"
      },
      "source": [
        "# **Task 4 - Model Evaluation and Accuracy Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNh4L0s7bgsp"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(labelCol=\"fraud_bool\", predictionCol=\"prediction\",metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(Lpredictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yyEFuCxyT5t"
      },
      "source": [
        "# **Task 5 - Results Visualization or Printing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P88D0MX1QyXO"
      },
      "outputs": [],
      "source": [
        "true_values = Lpredictions.select('fraud_bool').rdd.flatMap(lambda x: x).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3no4eIORW1i"
      },
      "outputs": [],
      "source": [
        "predicted_values = Lpredictions.select('prediction').rdd.flatMap(lambda x: x).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzWiTAYTR7xJ"
      },
      "outputs": [],
      "source": [
        "# Plot true vs predicted values\n",
        "plt.scatter(true_values, predicted_values, alpha=0.5)\n",
        "plt.plot(true_values, true_values, color='red', linestyle='--')  # Plotting y = x line for reference\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('True vs Predicted Values (Linear Regression)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA0fkmCeHWRN"
      },
      "source": [
        "# 3rd different method with same dataset  Model Selection and Implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJqN9xBn7a5x"
      },
      "outputs": [],
      "source": [
        "# Import Library ClusteringEvaluator and KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.clustering import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7IgW9PPUOIJ"
      },
      "outputs": [],
      "source": [
        "# init Kmeans and Train Modal\n",
        "kmeans = KMeans()\n",
        "CModel = kmeans.fit(rfFinalData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU2bbGc7SxnX"
      },
      "outputs": [],
      "source": [
        "# Define the evaluator\n",
        "evaluator = ClusteringEvaluator(metricName=\"silhouette\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOUMkT3-bvJo"
      },
      "outputs": [],
      "source": [
        "# Get the best model from cross-validation\n",
        "best_model = cv_model.bestModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZanvwGHz9WQe"
      },
      "source": [
        "# **Task 3 - Model Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9emZoRJSmYH"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "param_grid = ParamGridBuilder().addGrid(kmeans.k, range(2, 11)).addGrid(kmeans.maxIter, [10, 20, 30]).build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO2BcQ0nRXWq"
      },
      "outputs": [],
      "source": [
        "# Define cross-validation\n",
        "crossval = CrossValidator(estimator=kmeans,\n",
        "                          estimatorParamMaps=param_grid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDUVoF6hS7MT"
      },
      "outputs": [],
      "source": [
        "# Fit the cross-validator\n",
        "cv_model = crossval.fit(data.limit(30000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOmTlnIwrXVE"
      },
      "outputs": [],
      "source": [
        "# Get the best model from cross-validation\n",
        "best_model = cv_model.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1acQ8w4bwqq"
      },
      "outputs": [],
      "source": [
        "# Print the best parameters\n",
        "print(\"Best number of clusters (k):\", best_model.getOrDefault(\"k\"))\n",
        "print(\"Best max iterations:\", best_model.getOrDefault(\"maxIter\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uet9e1PDgcx6"
      },
      "source": [
        "# **Task 4 - Model Evaluation and Accuracy Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbQMN6NXb4K4"
      },
      "outputs": [],
      "source": [
        "# Evaluate clustering performance on the test data\n",
        "CPredictions = best_model.transform(data)\n",
        "silhouette_score = evaluator.evaluate(CPredictions)\n",
        "print(\"Silhouette score:\", silhouette_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x212sw5H2MnT"
      },
      "source": [
        "# **Task 5 - Results Visualization or Printing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPYAUzlRSD3F"
      },
      "outputs": [],
      "source": [
        "# Get prediction with all cluster\n",
        "cluster_labels = CPredictions.select('prediction').rdd.flatMap(lambda x: x).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGIb6y5USo94"
      },
      "outputs": [],
      "source": [
        "# Select fraud_bool and prediction column\n",
        "feature_cols = [\"fraud_bool\", \"prediction\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VclcAA_tSrY0"
      },
      "outputs": [],
      "source": [
        "# Extract feature values\n",
        "feature_values = CPredictions.select(feature_cols[0], feature_cols[1]).collect()\n",
        "feature_values = np.array(feature_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ob3-5U0TjZW"
      },
      "outputs": [],
      "source": [
        "# Plot clusters\n",
        "plt.scatter(feature_values[:, 0], feature_values[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
        "plt.xlabel(feature_cols[0])\n",
        "plt.ylabel(feature_cols[1])\n",
        "plt.title('Clustering Results (KMeans)')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIM6uLApSxi2"
      },
      "source": [
        "---\n",
        "\n",
        "# **Task 7 - Report HTML template**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrQu11N_DCfZ"
      },
      "outputs": [],
      "source": [
        "# install nbconvert (if facing the conversion error)\n",
        "!pip3 install nbconvert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReZWnCYdDH66"
      },
      "outputs": [],
      "source": [
        "# convert ipynb to html and submit this HTML file\n",
        "\n",
        "!jupyter nbconvert --to html /content/drive/MyDrive/Group_27_CRWK_CN7030.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}